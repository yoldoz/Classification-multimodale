{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Projet MI203: Classification multimodale\n<font size=\"3\">réalisé par: **Mouin BEN AMMAR & Yoldoz TABEI**</font><br>","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n<font size=\"3\">Afin de pouvoir classifier ces 9 environnements: Fôret, cité, plage, restaurant, salle de cours, rivière,jungle, supermarché et match de football, des données images et audio nous ont été fournies afin de les analyser et de classifier ces environnements à partir d'une approche multimodale.<br>\nDans ce projet, nous allons établir une démarche pour trouver les bonnes caractéristiques et nous combinerons les ressources afin de voir si le multimodal apporte à la classification.</font>","metadata":{}},{"cell_type":"markdown","source":"## Démarche\n<font size=\"3\">Tout d'abord, nous avons décidé de travailler sur les deux types de data séparemment. Pour les images, nous avons implémenté un réseau CNN, alors que pour l'audio nous avons choisi d'adopter un SVM.<br>\nAprès avoir fait le traitement et toutes les améliorations possibles sur les images, nous avons eu comme meilleure performance 99.3%. Quant à l'audio, le résultat était 94.6%. Pour ces deux types de data, nous avons déterminé tous les paramètres des modèles et de l'architecture CNN en utilisant des validation croisées .<br>\nEnsuite, nous avons essayé de créer une classification hybrides des deux modèles.<br>\nD'abord, on fait passer les images à travers notre réseau CNN pour extraire les prédictions sous forme d'un \"One hot vector\" de dimension 9. Ensuite, on procède par concaténer ces prédictions au vecteur représentant l'audio et on passe ce vecteur dans le SVM pour faire la séparation finale des données. En fait en ajoutant ce vecteur à l'audio, c'est comme ci on a créé un espace creux \"Sparce space\" de dimension 9 qui aide le SVM à mieux classifier les données, qui ont été déjà classifiées à 99,3% en utilisant le CNN, ce qui s'est avéré vrai, comme le résultat final obtenu est 99,768%.<br>","metadata":{}},{"cell_type":"markdown","source":"## 1. Importation des données","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndatadir = '/kaggle/input/multimodal-classification-2021-mi203/data'\n#datadir = 'data'\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ndata_df = pd.read_csv(os.path.join(datadir,'data_train.csv'), delimiter=',', nrows = None)\ndata = np.array(data_df)\n\nlabels = data[:,-1].astype('int32')\n\naudio = data[:, 1:-1].astype('float32')\n\nimg_list = data_df['IMAGE']\n\ndatadir_test = '/kaggle/input/multimodal-classification-2021-mi203/data'\ndata_df = pd.read_csv(os.path.join(datadir_test,'data_test_novt.csv'), delimiter=',', nrows = None)\ndata_test = np.array(data_df)\nlabels_test = data[:,-1].astype('int32')\n\naudio_test = data[:, 1:-1].astype('float32')\n\nimg_list_test = data_df['IMAGE']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Activation du GPU \n<font size=\"3\">Afin de minimiser le temps de l'exécution, on utilise le GPU qui permet de diviser ce temps par 12.5 par rapport à celui lors de l'activation du CPU.</font>","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\n\n#initializing random seeds\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Visualisation d'une image\n<font size=\"3\">Nous allons ici afficher une image à partir des données fournies.</font>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# image index\nidx = 75\n\nclass_list = ['FOREST', 'CITY', 'BEACH', 'CLASSROOM', 'RIVER', 'JUNGLE', 'RESTAURANT', 'GROCERY-STORE', 'FOOTBALL-MATCH']\nimg = Image.open(os.path.join(datadir, img_list.iloc[idx]))\nplt.imshow(np.asarray(img))\nprint(class_list[labels[idx]])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Dataloader\n\n<font size=\"3\">Etant donné que notre mémoire risque de ne pas avoir assez d'espace, le package Pytorch permet d'envoyer les données directement vers le modèle d'apprentissage profond: Le GPU. Ceci rend le traitement des données dans la phase d'entraînement plus efficace. On a également fait de la \"data augmentation\" en utilisant les transformations d'image(Resize, RandomCrop, ColorJitter, RandomRotation, RandomHorizontalFlip)</font>\n\n","metadata":{}},{"cell_type":"code","source":"\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nclass ImageDataset(Dataset):\n    def __init__(self, root_dir, files, labels=None, img_transform=None):\n        self.root_dir = root_dir\n        self.files = files\n     \n        self.labels = labels\n        self.img_transform = img_transform\n       \n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(self.root_dir, self.files.iloc[idx]))\n       \n        if self.img_transform is not None:\n            img = self.img_transform(img)\n       \n        if self.labels is not None:\n            return img,  int(self.labels[idx])\n        else:\n            return img\n\nimport torchvision\n\nimg_list_transform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224,224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\nimg_list_transform = torchvision.transforms.Compose([\n   \n    torchvision.transforms.Resize((224,224)),\n    torchvision.transforms.RandomCrop((222,222)),\n    torchvision.transforms.ColorJitter(brightness=0.2),\n    torchvision.transforms.RandomRotation(degrees=10),\n    torchvision.transforms.RandomHorizontalFlip(0.5),\n   \n    \n    torchvision.transforms.ToTensor(),\n    \n    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimg_dataset = ImageDataset(root_dir=datadir,\n                               files=img_list,\n                               \n                                 labels=labels,\n                              img_transform=img_list_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Training validation split\n<font size=\"3\">On définit la taille du \"batch\" à 112. C'est le nombre d'échantillons qui seront propagés à travers le réseau. Après avoir identifié la meilleure architecture et les hyperparamètres, on a mis le code qui divise notre Dataset en validation et training en commentaire vue le nombre limité de training data. </font>","metadata":{}},{"cell_type":"code","source":"#from torch.utils.data.sampler import SubsetRandomSampler\nbatch_size = 112\n#validation_split = .1\nshuffle_dataset = True\nrandom_seed= 42\n\n# Creating data indices for training and validation splits:\n#dataset_size = len(img_dataset)\n#indices = list(range(dataset_size))\n#split = int(np.floor(validation_split * dataset_size))\n#if shuffle_dataset :\n   # np.random.seed(random_seed)\n   # np.random.shuffle(indices)\n#train_indices, val_indices = indices[split:], indices[:split]\n\n# Creating PT data samplers and loaders:\n#train_sampler = SubsetRandomSampler(train_indices)\n#valid_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = torch.utils.data.DataLoader(img_dataset, batch_size=batch_size, \n                                           num_workers=2)\n#validation_loader = torch.utils.data.DataLoader(img_dataset, batch_size=batch_size,\n                                               # sampler=valid_sampler,num_workers=2)\n\n\nimg_dataset_test = ImageDataset(root_dir=datadir,\n                               files=img_list_test,\n                            \n                                \n                              img_transform=img_list_transform,\n                                 )\ntest_loader = torch.utils.data.DataLoader(img_dataset_test, batch_size=batch_size, \n                                           num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Charger un réseau pré-appris CNN\n\n<font size=\"3\">Tout d'abord, nous avons importé le réseau resnext50_32x4d. Ensuite, nous avons essayé plusieurs architectures, mais finalement nous avons décidé de supprimer la dernière couche et l'avons remplacé par notre propre architecture vu que cette dernière a abouti à une meilleure performance.</font>","metadata":{}},{"cell_type":"code","source":"import torchvision.models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nresnext = torchvision.models.resnext50_32x4d(pretrained=True, progress=True)\n#identify number of features in the last layer in order to extend the model\nnum_ftrs = resnext.fc.in_features\n\n\nif device.type == 'cuda':\n    resnext = resnext.cuda()\n\nmodel = resnext\n\nclass MyModel(nn.Module):\n            def __init__(self):\n                super(MyModel, self).__init__()\n                self.features = nn.Sequential(\n                    # excludes the last layer\n                    *list((model.children()))[:-1]\n                    \n                )\n                #dont train on the original model\n                for p in self.features :\n                         p.requires_grad = False\n                 #layers chosen to add       \n                self.linear1 = nn.Linear(num_ftrs,32 )\n               \n                self.linear2 = nn.Linear(32,256 )\n                \n                self.linear3 = nn.Linear(256,32 )\n               \n                self.linear4 = nn.Linear(32,9 )\n                \n               \n                \n            def forward(self, x):\n                x = self.features(x)\n                x = x.view(-1,num_ftrs)\n                x=F.leaky_relu(self.linear1(x))\n               \n                x=F.leaky_relu(self.linear2(x))\n                x=F.leaky_relu(self.linear3(x))\n                x=F.leaky_relu(self.linear4(x))\n                \n                return x\n\nmodel = MyModel()\n\n\n#print(mymodel)\nmodel.to('cuda:0')\nprint(\"      \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Training our network\n\n<font size=\"3\">Afin de ne pas avoir un overfitting, nous avons choisi un petit nombre d'Epochs. A la fin de chaque Epoch, on affiche l'\"accuracy\" du réseau pour bien se renseigner sur l'évolution des performances.<br>\nOn a tout d'abord décidé d'implémenter une méthode de \"Early stopping\" afin de mieux généraliser, mais les résultats montrent que le réseau généralise bien, même avec des performances supérieures à 99%.</font>\n","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\n\noptimizerCNN = optim.AdamW(model.parameters(), lr=0.00009)\ncriterion = nn.CrossEntropyLoss()\nnbepoch = 5\n\n\nfor epoch in range(nbepoch):\n    train_acc=0\n    model.train()\n    print(\"epoch\", epoch)\n    i=0\n    for i, data in enumerate(tqdm(train_loader)):\n        i+=1\n        inputs,  targets = data\n        inputs, targets = inputs.cuda(),targets.cuda()\n        optimizerCNN.zero_grad()\n        mespredictions = model(inputs)   \n        loss = criterion(mespredictions,targets)    \n\n        loss.backward() \n        optimizerCNN.step() \n        \n        out, inds = torch.max(mespredictions,dim=1)\n        train_acc += torch.sum(inds == targets)\n       \n        \n        \n        if random.randint(0,90)==0:\n            print(\"\\tloss=\",loss) \n       \n    final_train_acc = train_acc/(i*batch_size)\n    \n    \n    #EARLY STOPPING (if accurracy at the end of epoch is more than 96%)\n    \n    #if(final_train_acc>0.96):\n        #break\n        \n        \n    print(final_train_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Validation","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">On observe la performance du CNN sur l'ensemble de validation , on obtient en général la même performance (accuracy) que celle du \"training\".</font>","metadata":{}},{"cell_type":"code","source":"#from tqdm import tqdm\n\n#feat = []\n#i=0\n#val_acc=0\n#for i, data in enumerate(tqdm(validation_loader)):\n#    i+=1\n#    img,  targets = data\n    #print(targets)\n#    if device.type == 'cuda':\n#        img, targets = img.cuda(),  targets.cuda()\n#    with torch.no_grad():\n#        outputs = model(img)\n#    out, inds = torch.max(outputs,dim=1)\n#    val_acc += torch.sum(inds == targets)\n   \n#    if device.type == 'cuda':\n#        feat.append(outputs.cpu().numpy().squeeze())\n#    else:\n#        feat.append(outputs.numpy().squeeze())\n#final_val_acc = val_acc/(i*batch_size)\n#print(final_train_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Importation des données audio","metadata":{}},{"cell_type":"code","source":"traindata = pd.read_csv(os.path.join(datadir,'data_train.csv'), delimiter=',', nrows = None)\ndata = np.array(traindata)\n\ny_train = data[:,-1].astype('int32')\n\naudio_train = data[:, 1:-1]\n\n\ntestdata = pd.read_csv(os.path.join(datadir,'data_test_novt.csv'), delimiter=',', nrows = None)\ndata = np.array(testdata)\n\naudio_test = np.array(data[:, 1:], dtype='float64')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Extracting model output\n\n<font size=\"3\">On itère sur notre Train_loader, puis on extrait les prédictions données par le modèle, pour ensuite les concaténer avec les vecteurs audio et créer nos données hybrides.</font>","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nfeat = []\ni=0\nval_acc=0\nmodel.eval()\nfor i, data in enumerate(tqdm(train_loader)):\n    i+=1\n    img, targets = data\n    #print(targets)\n    if device.type == 'cuda':\n        img, targets = img.cuda(), targets.cuda()\n    with torch.no_grad():\n        outputs = model(img)\n    out, inds = torch.max(outputs,dim=1)\n    val_acc += torch.sum(inds == targets)\n    \n    if device.type == 'cuda':\n        feat.append(outputs.cpu().numpy().squeeze())\n    else:\n        feat.append(outputs.numpy().squeeze())\nfinal_val_acc = val_acc/(i*batch_size)\nprint(final_train_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 15. Concaténation des données du training","metadata":{}},{"cell_type":"code","source":"imgfeat = np.concatenate(feat)\n\nnp.save(os.path.join('/kaggle/working/', 'img_feat'), imgfeat)\nprint(imgfeat.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11. Transformation de l'audio","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(audio_train)\n\naudio_train_scaled = scaler.transform(audio_train)\naudio_test_scaled = scaler.transform(audio_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12. Concaténation de l'audio et des prédictions des images ","metadata":{}},{"cell_type":"code","source":"train_hh = np.concatenate((imgfeat,audio_train_scaled),axis=1)\nprint(train_hh.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13. SVM appliqué sur audio-image data \n","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">On a mis le code qui divise notre data en training et validation en commentaire pour les mêmes raisons qu'avant.</font>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Pour séparer les données en apprentissage et test\nfrom sklearn.model_selection import train_test_split\n\n#Xn, Xv, yn, yv = train_test_split(train_hh, y_train, random_state=42,test_size=0.1)\n\nfrom sklearn import svm\n\n\nsvc = svm.SVC(kernel='rbf', max_iter=-1, verbose = True,gamma='scale',\n             tol=1e-4,C=10.4,decision_function_shape='ovr')\nsvc.fit(train_hh, y_train)\n\nscore_train = svc.score(train_hh, y_train)\n#score_test = svc.score(Xv, yv)\n#print(c)\nprint(\"Taux de reco train=  {:.2f}%\".format(score_train*100))\n#print(\"Taux de reco =  {:.2f}%/{:.2f}% \".format(score_train*100,score_train*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 14. Extracting model output\n\n<font size=\"3\">On itère sur notre Test_loader, puis on extrait les prédictions données par le modèle, pour ensuite les concaténer avec les vecteurs audio et créer nos données hybrides.</font>","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nfeat_test = []\nmodel.eval()\nfor i, data in enumerate(tqdm(test_loader)):\n\n    img = data\n    #print(targets)\n    if device.type == 'cuda':\n        img = img.cuda()\n    with torch.no_grad():\n        outputs = model(img)\n   \n   \n    \n    if device.type == 'cuda':\n        feat_test.append(outputs.cpu())\n    else:\n        feat_test.append(outputs.numpy().squeeze())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 15. Concaténation audio-image Test","metadata":{}},{"cell_type":"code","source":"imgfeat_test = np.concatenate(feat_test)\ntest_hh=np.concatenate((imgfeat_test,audio_test_scaled),axis=1)\n\nprint(test_hh.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 16. Prédiction finale du SVM","metadata":{}},{"cell_type":"code","source":"y_pred = svc.predict(test_hh)\nprint(y_pred.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 17. Création du fichier de soumission","metadata":{}},{"cell_type":"code","source":"\nsubmission = pd.DataFrame({'CLASS':y_pred})\nsubmission=submission.reset_index()\nsubmission = submission.rename(columns={'index': 'Id'})\nsubmission.to_csv('My_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}